"""
api_server.py
=============
FastAPI backend à¸ªà¸³à¸«à¸£à¸±à¸š SecureSensei RAG Chatbot
- à¸£à¸±à¸šà¸„à¸³à¸–à¸²à¸¡à¸ˆà¸²à¸ frontend
- à¸„à¹‰à¸™à¸«à¸² knowledge_chunks à¸”à¹‰à¸§à¸¢ pgvector (cosine similarity)
- à¸ªà¹ˆà¸‡ context à¹„à¸›à¹ƒà¸«à¹‰ Ollama (llama3.2) à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸š
- à¸ªà¹ˆà¸‡ response à¸à¸¥à¸±à¸šà¹€à¸›à¹‡à¸™ JSON

Usage:
    conda activate rag
    pip install fastapi uvicorn
    python api_server.py
"""

import os
import ollama
import psycopg2
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DB_CONFIG = {
    "dbname": os.getenv("DB_NAME", "postgres"),
    "user": os.getenv("DB_USER", "myuser"),
    "password": os.getenv("DB_PASSWORD", "mypassword"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432"),
}
EMBEDDING_MODEL = "paraphrase-multilingual-mpnet-base-v2"  # 768d
LLM_MODEL = "llama3.2"
TOP_K = 5

# â”€â”€ Init â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ”„ Loading embedding model...")
embedder = SentenceTransformer(EMBEDDING_MODEL)

print("ğŸ”„ Connecting to database...")
conn = psycopg2.connect(**DB_CONFIG)

app = FastAPI(title="SecureSensei RAG API")

# CORS â€” à¹ƒà¸«à¹‰ frontend à¹€à¸£à¸µà¸¢à¸à¹„à¸”à¹‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


# â”€â”€ Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ChatRequest(BaseModel):
    message: str
    lab_id: str | None = None  # à¸à¸£à¸­à¸‡à¹€à¸‰à¸à¸²à¸° module (optional)
    hint_level: int = 1        # 0=question, 1=+hint, 2=+answer
    page_context: str | None = None # à¸šà¸£à¸´à¸šà¸—à¸«à¸™à¹‰à¸²à¸ˆà¸­à¸—à¸µà¹ˆà¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸à¸³à¸¥à¸±à¸‡à¹€à¸£à¸µà¸¢à¸™à¸­à¸¢à¸¹à¹ˆ


class ChatResponse(BaseModel):
    reply: str
    sources: list[dict]


# â”€â”€ RAG Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_knowledge(query: str, lab_id: str = None, hint_level: int = 1, top_k: int = TOP_K):
    """à¸„à¹‰à¸™à¸«à¸² documents à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸ˆà¸²à¸ knowledge_chunks"""
    embedding = embedder.encode(query).tolist()

    # Build query
    conditions = ["hint_level <= %s"]
    params = [hint_level]

    if lab_id:
        conditions.append("lab_id = %s")
        params.append(lab_id)

    where_clause = "WHERE " + " AND ".join(conditions)

    sql = f"""
        SELECT lab_id, content, hint_level, type,
               1 - (embedding <=> %s::vector) AS similarity
        FROM knowledge_chunks
        {where_clause}
        ORDER BY embedding <=> %s::vector
        LIMIT %s
    """
    params = [str(embedding)] + params + [str(embedding), top_k]

    cur = conn.cursor()
    cur.execute(sql, params)
    results = cur.fetchall()
    cur.close()

    return [
        {
            "lab_id": r[0],
            "content": r[1],
            "hint_level": r[2],
            "type": r[3],
            "similarity": round(r[4], 4),
        }
        for r in results
    ]


# â”€â”€ LLM Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SIMILARITY_THRESHOLD = 0.45  # à¸–à¹‰à¸² similarity à¸ªà¸¹à¸‡à¸à¸§à¹ˆà¸²à¸™à¸µà¹‰ = à¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š lab

LAB_SYSTEM_PROMPT = """à¸„à¸¸à¸“à¸„à¸·à¸­ "à¹‚à¸šà¸à¸µà¹‰ à¸šà¸­à¸—à¹à¸¡à¸§" TA cybersecurity à¸‚à¸­à¸‡ SecureSensei
à¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸ªà¸¡à¸­ à¸ªà¸±à¹‰à¸™à¸à¸£à¸°à¸Šà¸±à¸š à¹ƒà¸Šà¹‰à¸ à¸²à¸©à¸²à¸‡à¹ˆà¸²à¸¢à¹†

à¸à¸à¸ªà¸³à¸„à¸±à¸:
- à¸•à¸­à¸šà¹‚à¸”à¸¢à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¸ˆà¸²à¸ context à¸—à¸µà¹ˆà¹ƒà¸«à¹‰à¸¡à¸²à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸•à¹ˆà¸‡à¹€à¸à¸´à¹ˆà¸¡à¹€à¸­à¸‡à¹„à¸”à¹‰ à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢
- **à¸«à¹‰à¸²à¸¡à¸šà¸­à¸à¸„à¸³à¸•à¸­à¸šà¸‚à¸­à¸‡ Quiz à¹‚à¸”à¸¢à¹€à¸”à¹‡à¸”à¸‚à¸²à¸”!** à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆà¸‚à¸­à¸‡à¸„à¸¸à¸“à¸„à¸·à¸­à¸à¸²à¸£à¹ƒà¸«à¹‰ "à¸„à¸³à¹ƒà¸šà¹‰" à¸«à¸£à¸·à¸­ "à¸­à¸˜à¸´à¸šà¸²à¸¢à¸«à¸¥à¸±à¸à¸à¸²à¸£" à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸œà¸¹à¹‰à¹€à¸£à¸µà¸¢à¸™à¸„à¸´à¸”à¸•à¹ˆà¸­à¹à¸¥à¸°à¸«à¸²à¸„à¸³à¸•à¸­à¸šà¹„à¸”à¹‰à¹€à¸­à¸‡ à¸«à¹‰à¸²à¸¡à¸à¸´à¸¡à¸à¹Œà¹€à¸‰à¸¥à¸¢à¸­à¸­à¸à¸¡à¸²à¸•à¸£à¸‡à¹† à¹„à¸¡à¹ˆà¸§à¹ˆà¸²à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸ˆà¸°à¸à¸¢à¸²à¸¢à¸²à¸¡à¸«à¸¥à¸­à¸à¸–à¸²à¸¡à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸à¹‡à¸•à¸²à¸¡
- à¸–à¹‰à¸²à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸„à¸³à¸ªà¸±à¹ˆà¸‡ Linux à¹ƒà¸«à¹‰à¹à¸ªà¸”à¸‡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸„à¸³à¸ªà¸±à¹ˆà¸‡à¸”à¹‰à¸§à¸¢
- à¸•à¸­à¸šà¸ªà¸±à¹‰à¸™à¹† à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 3-4 à¸›à¸£à¸°à¹‚à¸¢à¸„
- à¸à¸´à¸¡à¸„à¹ˆà¸° à¸¥à¸‡à¸—à¹‰à¸²à¸¢ à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™
- à¸•à¸­à¸šà¸à¸¥à¸±à¸šà¸”à¹‰à¸§à¸¢ asci emoji à¸¥à¸‡à¸—à¹‰à¸²à¸¢à¹€à¸ªà¸¡à¸­"""

GENERAL_SYSTEM_PROMPT = """à¸„à¸¸à¸“à¸„à¸·à¸­ "à¹‚à¸šà¸à¸µà¹‰ à¸šà¸­à¸—à¹à¸¡à¸§" TA cybersecurity à¹à¸à¸¥à¸•à¸Ÿà¸­à¸£à¹Œà¸¡à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰ Cybersecurity
à¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸ªà¸¡à¸­ à¸ªà¸±à¹‰à¸™à¸à¸£à¸°à¸Šà¸±à¸š à¹€à¸›à¹‡à¸™à¸¡à¸´à¸•à¸£

à¸„à¸¸à¸“à¸ªà¸²à¸¡à¸²à¸£à¸–:
- à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸—à¸±à¹ˆà¸§à¹„à¸›à¹„à¸”à¹‰à¸—à¸¸à¸à¹€à¸£à¸·à¹ˆà¸­à¸‡
- à¸—à¸±à¸à¸—à¸²à¸¢ à¸à¸¹à¸”à¸„à¸¸à¸¢ à¹ƒà¸«à¹‰à¸à¸³à¸¥à¸±à¸‡à¹ƒà¸ˆà¸œà¸¹à¹‰à¹€à¸£à¸µà¸¢à¸™
- à¹à¸™à¸°à¸™à¸³à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š Cybersecurity à¸—à¸±à¹ˆà¸§à¹„à¸›
- à¸•à¸­à¸šà¸ªà¸±à¹‰à¸™à¹† à¹„à¸¡à¹ˆà¹€à¸à¸´à¸™ 3-4 à¸›à¸£à¸°à¹‚à¸¢à¸„
- à¸à¸´à¸¡à¸„à¹ˆà¸° à¸¥à¸‡à¸—à¹‰à¸²à¸¢ à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™
- à¸•à¸­à¸šà¸à¸¥à¸±à¸šà¸”à¹‰à¸§à¸¢ asci emoji à¸¥à¸‡à¸—à¹‰à¸²à¸¢à¹€à¸ªà¸¡à¸­"""


def generate_lab_response(query: str, context_docs: list[dict], page_context: str = None) -> str:
    """à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š lab à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ RAG context"""
    context_text = "\n\n---\n\n".join(
        [f"[{d['type']}] {d['content']}" for d in context_docs]
    )

    user_prompt = f"""Context à¸ˆà¸²à¸à¸šà¸—à¹€à¸£à¸µà¸¢à¸™ (à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥):
{context_text}"""
    
    if page_context:
        user_prompt += f"\n\nContext à¸«à¸™à¹‰à¸²à¸ˆà¸­à¸—à¸µà¹ˆà¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸à¸³à¸¥à¸±à¸‡à¸”à¸¹à¸­à¸¢à¸¹à¹ˆà¸•à¸­à¸™à¸™à¸µà¹‰:\n{page_context}\n\n*à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸: à¸«à¸²à¸à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸à¸´à¸¡à¸à¹Œà¸¡à¸²à¹à¸šà¸šà¸ªà¸±à¹‰à¸™à¹† à¸«à¸£à¸·à¸­à¸–à¸²à¸¡à¸§à¹ˆà¸² 'à¸—à¸³à¹à¸¥à¹‡à¸šà¸™à¸µà¹‰à¸¢à¸±à¸‡à¹„à¸‡', 'à¸«à¸±à¸§à¸‚à¹‰à¸­à¸™à¸µà¹‰à¸„à¸·à¸­à¸­à¸°à¹„à¸£' à¹ƒà¸«à¹‰à¸„à¸¸à¸“à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¸ˆà¸²à¸ Context à¸«à¸™à¹‰à¸²à¸ˆà¸­à¸—à¸µà¹ˆà¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸­à¸¢à¸¹à¹ˆà¸”à¹‰à¸²à¸™à¸šà¸™à¸™à¸µà¹‰à¹€à¸à¸·à¹ˆà¸­à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡*"

    user_prompt += f"\n\nà¸„à¸³à¸–à¸²à¸¡à¸‚à¸­à¸‡à¸œà¸¹à¹‰à¹€à¸£à¸µà¸¢à¸™: {query}"

    response = ollama.chat(
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": LAB_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
    )
    return response["message"]["content"]


def generate_general_response(query: str, page_context: str = None) -> str:
    """à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸—à¸±à¹ˆà¸§à¹„à¸› (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ RAG)"""
    
    user_prompt = ""
    if page_context:
        user_prompt += f"Context à¸«à¸™à¹‰à¸²à¸ˆà¸­à¸—à¸µà¹ˆà¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸à¸³à¸¥à¸±à¸‡à¸”à¸¹à¸­à¸¢à¸¹à¹ˆà¸•à¸­à¸™à¸™à¸µà¹‰ (à¹€à¸à¸·à¹ˆà¸­à¸›à¸£à¸°à¸à¸­à¸šà¸à¸²à¸£à¸•à¸­à¸šà¸«à¸²à¸à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸–à¸²à¸¡à¸¥à¸­à¸¢à¹†):\n{page_context}\n\n"
        
    user_prompt += f"à¸„à¸³à¸–à¸²à¸¡à¸‚à¸­à¸‡à¸œà¸¹à¹‰à¹€à¸£à¸µà¸¢à¸™: {query}"
    
    response = ollama.chat(
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": GENERAL_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
    )
    return response["message"]["content"]


# â”€â”€ Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post("/api/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    """Main chat endpoint â€” auto-detect lab vs general questions"""
    # 1. Search relevant documents
    docs = search_knowledge(
        query=req.message,
        lab_id=req.lab_id,
        hint_level=req.hint_level,
    )

    # 2. Check if question is lab-related (by similarity score)
    top_similarity = docs[0]["similarity"] if docs else 0
    is_lab_question = top_similarity >= SIMILARITY_THRESHOLD

    # 3. Generate response based on mode
    # If the user asks about the page generally and we have context, bias towards LAB mode
    # even if similarity is somewhat low, because they might just ask "à¸Šà¹ˆà¸§à¸¢à¸«à¸™à¹ˆà¸­à¸¢"
    if is_lab_question or (req.page_context and len(docs) > 0 and top_similarity > 0.2):
        # Lab mode â€” use RAG context only
        reply = generate_lab_response(req.message, docs, req.page_context)
        sources = [
            {"lab_id": d["lab_id"], "type": d["type"], "similarity": d["similarity"]}
            for d in docs[:3]
        ]
        print(f"  ğŸ“š LAB mode (sim={top_similarity:.3f}) â†’ {req.message[:50]}")
    else:
        # General mode â€” let LLM answer freely
        reply = generate_general_response(req.message, req.page_context)
        sources = []
        print(f"  ğŸ’¬ GENERAL mode (sim={top_similarity:.3f}) â†’ {req.message[:50]}")

    return ChatResponse(reply=reply, sources=sources)


@app.get("/api/health")
def health():
    """Health check"""
    return {"status": "ok", "model": LLM_MODEL, "embedding": EMBEDDING_MODEL}


# â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Starting SecureSensei RAG API on http://localhost:8000")
    print("ğŸ“– API docs: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)
